{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiH_Ld2k2hq9",
        "outputId": "7182cf8c-b504-4be0-bbf8-43a00421aba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Étape 0: Initialisation.\n",
            "\n",
            "Étape 1: Connexion au TPU...\n",
            "Device TPU trouvé: xla:0\n",
            "\n",
            "Étape 2: Téléchargement du tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1141579989.py:14: DeprecationWarning: Use torch_xla.device instead\n",
            "  device = xm.xla_device()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer téléchargé.\n",
            "\n",
            "Étape 3: Téléchargement du modèle...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1141579989.py:29: DeprecationWarning: Use torch_xla.sync instead\n",
            "  xm.mark_step()\n",
            "/tmp/ipython-input-1141579989.py:43: DeprecationWarning: Use torch_xla.sync instead\n",
            "  xm.mark_step()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modèle prêt sur TPU.\n",
            "\n",
            "Étape 4: Tokenisation du prompt...\n",
            "Prompt tokenizé.\n",
            "\n",
            "Étape 5: Warm-up TPU (compilation initiale courte)...\n",
            "Warm-up terminé.\n",
            "\n",
            "Étape 6: Génération manuelle token par token (optimisée pour TPU)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1141579989.py:66: DeprecationWarning: Use torch_xla.sync instead\n",
            "  xm.mark_step()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ 10 tokens générés...\n",
            "Hello, how are you today?\n",
            "I hope you are doing well.\n",
            "\n",
            "--------------------------------------------------\n",
            "→ 20 tokens générés...\n",
            "Hello, how are you today?\n",
            "I hope you are doing well.\n",
            "I am writing to you today to let you know\n",
            "--------------------------------------------------\n",
            "→ 30 tokens générés...\n",
            "Hello, how are you today?\n",
            "I hope you are doing well.\n",
            "I am writing to you today to let you know that I have been awarded a scholarship to study at\n",
            "--------------------------------------------------\n",
            "→ 40 tokens générés...\n",
            "Hello, how are you today?\n",
            "I hope you are doing well.\n",
            "I am writing to you today to let you know that I have been awarded a scholarship to study at the University of Oxford.\n",
            "I am very excited\n",
            "--------------------------------------------------\n",
            "→ 50 tokens générés...\n",
            "Hello, how are you today?\n",
            "I hope you are doing well.\n",
            "I am writing to you today to let you know that I have been awarded a scholarship to study at the University of Oxford.\n",
            "I am very excited about this news and I am looking forward to starting\n",
            "--------------------------------------------------\n",
            "→ 60 tokens générés...\n",
            "Hello, how are you today?\n",
            "I hope you are doing well.\n",
            "I am writing to you today to let you know that I have been awarded a scholarship to study at the University of Oxford.\n",
            "I am very excited about this news and I am looking forward to starting my studies there.\n",
            "I am also very happy\n",
            "--------------------------------------------------\n",
            "\n",
            "Génération terminée.\n",
            "\n",
            "Étape 7: Décodage final...\n",
            "\n",
            "--- RÉSULTAT FINAL ---\n",
            "Hello, how are you today?\n",
            "I hope you are doing well.\n",
            "I am writing to you today to let you know that I have been awarded a scholarship to study at the University of Oxford.\n",
            "I am very excited about this news and I am looking forward to starting my studies there.\n",
            "I am also very happy to be able to\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch_xla.core.xla_model as xm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(\"Étape 0: Initialisation.\")\n",
        "\n",
        "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
        "os.environ[\"TORCH_XLA_DEBUG\"] = \"0\"\n",
        "MODEL_ID = \"utter-project/EuroLLM-1.7B\"\n",
        "\n",
        "# --- ÉTAPE 1 ---\n",
        "print(\"\\nÉtape 1: Connexion au TPU...\")\n",
        "device = xm.xla_device()\n",
        "print(f\"Device TPU trouvé: {device}\")\n",
        "\n",
        "# --- ÉTAPE 2 ---\n",
        "print(\"\\nÉtape 2: Téléchargement du tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Tokenizer téléchargé.\")\n",
        "\n",
        "# --- ÉTAPE 3 ---\n",
        "print(\"\\nÉtape 3: Téléchargement du modèle...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16)\n",
        "model.config.use_cache = False  # XLA incompatible\n",
        "model.to(device)\n",
        "xm.mark_step()\n",
        "print(\"Modèle prêt sur TPU.\")\n",
        "\n",
        "# --- ÉTAPE 4 ---\n",
        "prompt = \"Hello, how are you today\"\n",
        "print(\"\\nÉtape 4: Tokenisation du prompt...\")\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "print(\"Prompt tokenizé.\")\n",
        "\n",
        "# --- ÉTAPE 5 ---\n",
        "print(\"\\nÉtape 5: Warm-up TPU (compilation initiale courte)...\")\n",
        "with torch.no_grad():\n",
        "    _ = model(input_ids)\n",
        "xm.mark_step()\n",
        "print(\"Warm-up terminé.\")\n",
        "\n",
        "# --- ÉTAPE 6 ---\n",
        "print(\"\\nÉtape 6: Génération manuelle token par token (optimisée pour TPU)...\")\n",
        "\n",
        "max_new_tokens = 64\n",
        "generated_ids = input_ids\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(max_new_tokens):\n",
        "        # Forward\n",
        "        outputs = model(generated_ids)\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        # Prendre le token le plus probable (greedy)\n",
        "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # Ajouter le nouveau token à la séquence\n",
        "        generated_ids = torch.cat([generated_ids, next_token], dim=-1)\n",
        "\n",
        "        # Synchronisation TPU\n",
        "        xm.mark_step()\n",
        "\n",
        "        # Vérifie si fin de séquence atteinte\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            print(f\"\\nArrêt précoce à {i+1} tokens (EOS atteint).\")\n",
        "            break\n",
        "\n",
        "        # Affiche un aperçu tous les 10 tokens\n",
        "        if (i + 1) % 10 == 0:\n",
        "            partial = tokenizer.decode(generated_ids[0].cpu(), skip_special_tokens=True)\n",
        "            print(f\"→ {i+1} tokens générés...\")\n",
        "            print(partial)\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nGénération terminée.\")\n",
        "\n",
        "# --- ÉTAPE 7 ---\n",
        "print(\"\\nÉtape 7: Décodage final...\")\n",
        "generated_text = tokenizer.decode(generated_ids[0].cpu(), skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- RÉSULTAT FINAL ---\")\n",
        "print(generated_text)"
      ]
    }
  ]
}